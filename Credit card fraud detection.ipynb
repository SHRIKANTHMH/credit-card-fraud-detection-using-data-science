{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf0f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f9504f",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0270de6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb02f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11963a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = df[df['Class'] == 1] \n",
    "print('Fraud Transactions:{}'.format(len(df[df['Class'] == 1]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = df[df['Class'] == 0]\n",
    "print('Valid Transactions: {}'.format(len(df[df['Class'] == 0]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9029305",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount details of the fraudulent transaction\") \n",
    "fraud.Amount.describe() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"details of valid transaction\") \n",
    "valid.Amount.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f932c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e99cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('creditcard.csv')\n",
    "X = df.drop(columns=['Class'], axis = 1) \n",
    "Y = df[\"Class\"] \n",
    "print(X.shape) \n",
    "print(Y.shape)  \n",
    "xData = X.values \n",
    "yData = Y.values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "xTrain, xTest, yTrain, yTest = train_test_split(xData, yData, test_size = 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce62cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "rfc = RandomForestClassifier() \n",
    "rfc.fit(xTrain, yTrain) \n",
    "yPred = rfc.predict(xTest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d769f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "LABELS = ['Normal', 'Fraud'] \n",
    "conf_matrix =confusion_matrix(yTest, yPred) \n",
    "plt.figure(figsize =(12, 12)) \n",
    "sns.heatmap(conf_matrix, xticklabels = LABELS, \n",
    "yticklabels = LABELS, annot = True, fmt =\"d\"); \n",
    "plt.title(\"Confusion matrix\") \n",
    "plt.ylabel('True class') \n",
    "plt.xlabel('Predicted class') \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4772e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score  \n",
    "from sklearn.metrics import precision_score, recall_score \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "  \n",
    " \n",
    "print(\"The model used is Random Forest classifier\") \n",
    "  \n",
    "acc = accuracy_score(yTest, yPred) \n",
    "print(\"The accuracy is {}\".format(acc)) \n",
    "  \n",
    "prec = precision_score(yTest, yPred) \n",
    "print(\"The precision is {}\".format(prec)) \n",
    "  \n",
    "rec = recall_score(yTest, yPred) \n",
    "print(\"The recall is {}\".format(rec)) \n",
    "  \n",
    "f1 = f1_score(yTest, yPred) \n",
    "print(\"The F1-Score is {}\".format(f1)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(xTrain,yTrain)\n",
    "y1Preds = lr.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76862e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score  \n",
    "from sklearn.metrics import precision_score, recall_score \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "print(\"The model used is Logistic Regression Classifier\") \n",
    "  \n",
    "acc = accuracy_score(yTest, y1Preds) \n",
    "print(\"The accuracy is {}\".format(acc)) \n",
    "  \n",
    "prec = precision_score(yTest, y1Preds) \n",
    "print(\"The precision is {}\".format(prec)) \n",
    "  \n",
    "rec = recall_score(yTest, y1Preds) \n",
    "print(\"The recall is {}\".format(rec)) \n",
    "  \n",
    "f1 = f1_score(yTest, y1Preds) \n",
    "print(\"The F1-Score is {}\".format(f1)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f899fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=25)\n",
    "knn.fit(xTrain, yTrain)\n",
    "y2Preds = knn.predict(xTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers = len(fraud)\n",
    "n_errors = (y2Preds != yTest).sum()\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"The model used is Logistic Regression Classifier\")\n",
    "print(classification_report(yTest, y2Preds))\n",
    "acc = accuracy_score(yTest, y2Preds)\n",
    "print(\"The accuracy is {}\".format(acc))\n",
    "\n",
    "prec = precision_score(yTest, y2Preds)\n",
    "print(\"The precision is {}\".format(prec))\n",
    "\n",
    "rec = recall_score(yTest, y2Preds)\n",
    "print(\"The recall is {}\".format(rec))\n",
    "\n",
    "f1 = f1_score(yTest, y2Preds)\n",
    "print(\"The F1-Score is {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "s_model = svm.SVC(kernel = 'linear', random_state = 0)\n",
    "s_model.fit(xTrain,yTrain)\n",
    "y3preds = s_model.predict(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3078bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metricsimport classification_report\n",
    "print(\"The model used is SVM Classifier\")\n",
    "print(classification_report(yTest, y3preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c19f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
